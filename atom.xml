<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
  <title>qfBlog</title>
  <link href="http://blog.quantifind.com"/>
  <link type="application/atom+xml" rel="self" href="http://blog.quantifind.com/atom.xml"/>
  <updated>2013-12-11T12:10:22-06:00</updated>
  <id> http://blog.quantifind.com </id>
  <author>
    <name>quantiFind</name>
    <email>info@quantifind.com</email>
  </author>

  
  <entry>
    <id>http://blog.quantifind.com/posts/Sumac</id>
    <link type="text/html" rel="alternate" href="http://blog.quantifind.com/posts/Sumac"/>
    <title>Sumac</title>
    <published>2013-05-13T00:00:00-05:00</published>
    <updated>2013-05-13T00:00:00-05:00</updated>
    <author>
      <name></name>
    </author>
    <content type="html">&lt;p&gt;We&amp;rsquo;re proud to announce the release of &lt;strong&gt;&lt;a href=&quot;https://github.com/quantifind/Sumac&quot;&gt;Sumac&lt;/a&gt;&lt;/strong&gt;, our first open source library.
Sumac is a simple, lighweight library for parsing command line arguments in Scala.&lt;/p&gt;

&lt;!--more--&gt;


&lt;p&gt;It is available for
download on Sonatype; to use, add this to your sbt definitions:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&quot;com.quantifind&quot; % &quot;sumac_2.9.3&quot; % &quot;0.1&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Another Argument Parsing Library?&lt;/h2&gt;

&lt;p&gt;We know, there are already lots of argument parsing libaries out there, even scala specific ones.  Does the world really
need yet another one?&lt;/p&gt;

&lt;p&gt;Yes, it does.  Most libraries require ludicrous amounts of configuration to define even the simplest set of arguments.
Sumac gets rid of all the boilerplate.&lt;/p&gt;

&lt;p&gt;As an example, say you want to write a program that prints out all integers in a range along with their squares.  So you need
two arguments, &amp;ldquo;from&amp;rdquo; and &amp;ldquo;to&amp;rdquo;, both integers.  With Sumac, just make a class that extends &lt;code&gt;FieldArgs&lt;/code&gt; with appropriately
named and typed fields.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import com.quantifind.sumac.FieldArgs

class SquareArgs extends FieldArgs {
  var from: Int = _
  var to: Int = _
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can make a program that uses those arguments by extending &lt;code&gt;ArgMain&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import com.quantifind.sumac.{ArgMain, FieldArgs}

object PrintSquares extends ArgMain[SquareArgs] {
  def main(args: SquareArgs) {
    (args.from to args.to).foreach{i =&amp;gt;
      println(i + &quot;\t&quot; + (i*i))
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And we&amp;rsquo;re done!  Sumac automatically figures out the names &amp;amp; types of the arguments from the fields of &lt;code&gt;SquareArgs&lt;/code&gt;, using
reflection.  Take a second to appreciate how DRY this makes your code.  In your code, you already name your arguments and
 give them types.  You don&amp;rsquo;t need to repeat that information at all.  Nor do you get your arguments parsed into some
 weird &amp;ldquo;holder&amp;rdquo; object &amp;mdash; you wanted an &lt;code&gt;Int&lt;/code&gt;, so Sumac gave you an &lt;code&gt;Int&lt;/code&gt;.&lt;/p&gt;

&lt;h2&gt;Mixing Traits For Composable Arguments&lt;/h2&gt;

&lt;p&gt;So far so good, but we&amp;rsquo;ve basically just recreated functionality that already exists in Optional, another arg parsing
library.  In fact, Optional lets you put the argument names right in your definition of &lt;code&gt;main&lt;/code&gt;, without making you
create the helper &amp;ldquo;Args&amp;rdquo; class.&lt;/p&gt;

&lt;p&gt;But it turns out that having your arguments live in a well-defined class is a good thing, for a number of reasons.  The
most important one is that you can create standard sets of arguments using traits, then mix them together into different
Arg classes.&lt;/p&gt;

&lt;p&gt;For example, lets say that you often find yourself in need of one or more of:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A connection to &lt;a href=&quot;http://mongodb.org&quot;&gt;MongoDB&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A &lt;a href=&quot;http://spark-project.org&quot;&gt;SparkContext&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A customer ID&lt;/li&gt;
&lt;li&gt;A location for data files&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;You might define the following traits:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;trait MongoArgs extends FieldArgs {
  var mongo: String = _
  def mongoConn: MongoConnection = ...
}

trait SparkContextArgs extends FieldArgs {
  var sparkMaster: String = _
  var sparkJobName: String = _
  def sparkContext: SparkContext = ...
}

trait CustomerArgs extends FieldArgs {
  var customerId: Int = _
}

trait DataFileArgs extends FieldArgs {
  var transactionLogDirs: List[String] = _
  var monthlyRollupDir: String = _
  def transactionLogRdd(sc: SparkContext) =
    sc.hadoopFile(transactionLogDirs.mkString(&quot;,&quot;))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then you might define a a few different programs that make use of these traits:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class CustomerReportArgs extends SparkContextArgs with CustomerArgs with DataFileArgs
object CustomerReport extends ArgMain[CustomerReportArgs] {
  def main(args: CustomerReportArgs) {
    val sc = args.sparkContext
    val logs = args.transactionLogRdd(sc)
    // ... use spark to analyze data for one customer ...
  }
}

class RollupBuilderArgs extends SparkContextArgs with DataFileArgs with MongoArgs {
  var useApproximateRollups: Boolean = false
}
object RollupBuilder extends ArgMain[RollupBuilderArgs] {
  def main(args: RollupBuilderArgs) {
    val sc = args.sparkContext
    val logs = sc.hadoopFile(args.transactionLogDir)
    val config = OurCustomConfig(args.mongoConnection)
    val rollups = if (args.useApproximateRollups) {
      // ...
    } else {
      // ...
    }
    rollups.saveAsTextFile(args.monthlyRollupDir + ...)
  }
}

class ShowCustomerMetaArgs extends CustomerArgs with MongoArgs
object ShowCustomerMeta extends ArgMain[ShowCustomerMetaArgs] {
  def main(args: ShowCustomerMetaArgs) {
    val config = OurCustomConfig(args.mongoConnection)
    println(config.getCustomerConfig(args.customerId))
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By defining the traits once, we get to&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Standardize the names for arguments.  After a few uses, we learn to always use &amp;ldquo;&amp;mdash;mongo &lt;blah&gt;&amp;rdquo;, not &amp;ldquo;&amp;mdash;mongoHost
&lt;blah&gt;&amp;rdquo; or &amp;ldquo;&amp;mdash;mongoDB &lt;blah&gt;&amp;rdquo;, even if its my first use of a particular class.&lt;/li&gt;
&lt;li&gt;Define common helper methods that are easily shared.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;We&amp;rsquo;re not limited to using fields defined in the standard traits &amp;mdash; individual programs can always add in
their own arguments.  For example, with &lt;code&gt;RollupBuilder&lt;/code&gt;  we can add the argument &amp;ldquo;&amp;mdash;useApproximateRollups true&amp;rdquo; to
 control its behavior.&lt;/p&gt;

&lt;p&gt;Of course, none of this would be possible without the power of traits in Scala :)&lt;/p&gt;

&lt;h2&gt;And That&amp;rsquo;s Not All!&lt;/h2&gt;

&lt;p&gt;There&amp;rsquo;s a lot more to Sumac:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Out-of-the-box support for parsing &lt;code&gt;String&lt;/code&gt;,&lt;code&gt;Int&lt;/code&gt;,&lt;code&gt;Long&lt;/code&gt;,&lt;code&gt;Float&lt;/code&gt;,&lt;code&gt;Double&lt;/code&gt;,&lt;code&gt;Boolean&lt;/code&gt;,&lt;code&gt;File&lt;/code&gt;,&lt;code&gt;Regex&lt;/code&gt;, as well as &lt;code&gt;List&lt;/code&gt;s and &lt;code&gt;Set&lt;/code&gt;s
whose type parameter is any of the other supported types.  (Well, actually you can only use a type parameter that extends
&lt;code&gt;AnyRef&lt;/code&gt;, because of the limitations of using java&amp;rsquo;s reflection.)&lt;/li&gt;
&lt;li&gt;Easily define your own custom parser for other types.  (We use this to parse joda &lt;code&gt;DateTime&lt;/code&gt; objects.)&lt;/li&gt;
&lt;li&gt;You can add custom validation logic, eg. to make sure &lt;code&gt;limit &amp;gt; 0&lt;/code&gt; or to make sure &lt;code&gt;start &amp;lt; end&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;If you don&amp;rsquo;t want the command line argument to have the same name as the field, you can customize it with an Annotation.&lt;/li&gt;
&lt;li&gt;The code is very modular.  In particular, definition of arguments is totally separate from parsing.  At quantifind,
we use this create internal tools which convert an argument list into a simple web UI.&lt;/li&gt;
&lt;li&gt;Using args objects leads to composable programs.  Its very easy to take a program which was written to be called from
the command line, and now call it from within another program, by just filling in the appropriate arg object.&lt;/li&gt;
&lt;li&gt;Super lightweight.  Its a small amount of scala code, with &lt;em&gt;no&lt;/em&gt; external dependencies at all.&lt;/li&gt;
&lt;li&gt;We&amp;rsquo;re working on adding support for pulling arguments out of &lt;a href=&quot;http://zookeeper.apache.org&quot;&gt;ZooKeeper&lt;/a&gt; in the &amp;ldquo;zk&amp;rdquo;
  module.  It&amp;rsquo;s already working, but hasn&amp;rsquo;t been tested much and api changes are still likely.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Open Source&lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;re really happy to be releasing this as open source as a way to give back.  Sumac is solving such a general purpose
problem that we want everyone to take advantage of it.  After all, we were inspired by Optional, another open source
project.  We use Sumac very extensively, have good units tests, and feel that its definitely ready
 to be integrated into production environments.&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s not to say its &amp;ldquo;complete&amp;rdquo; by any means &amp;mdash; there is a long list of features we&amp;rsquo;d still like to add when we get
around to it.  We&amp;rsquo;d love for the community as a whole to improve upon it by giving feedback and submitting pull requests.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <id>http://blog.quantifind.com/posts/spark-unit-test</id>
    <link type="text/html" rel="alternate" href="http://blog.quantifind.com/posts/spark-unit-test"/>
    <title>Unit Testing With Spark</title>
    <published>2013-01-04T00:00:00-06:00</published>
    <updated>2013-01-04T00:00:00-06:00</updated>
    <author>
      <name></name>
    </author>
    <content type="html">&lt;p&gt;One of the great things about Spark is the ability to use it on just one machine in &amp;ldquo;local mode.&amp;rdquo;  Not only is this useful
for trying out Spark before setting up a cluster, it makes it easy to use Spark in your unit tests.  It was easy enough
to write one test using Spark, but we ran into a couple of issues when we went to integrate them into test suite:&lt;/p&gt;

&lt;!--more--&gt;


&lt;p&gt;&lt;em&gt;&lt;strong&gt;UPDATE 12/07/13:&lt;/strong&gt; This was originally written for use with Spark 0.6.  Spark has changed the API slightly since then, so
the code here is out of date.  Nonetheless, these tips are still useful &amp;ldquo;in spirit&amp;rdquo;, though you&amp;rsquo;ll
need to update the code samples to get them to work.&lt;/em&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Isolation&lt;/em&gt;.  If one test was broken, we didn&amp;rsquo;t want it to mess up the SparkContext for other tests.  Each test
should get its own clean SparkContext.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Limited Logging&lt;/em&gt;.  The logs from Spark are great while you&amp;rsquo;re actively debugging a test.  But they are so verbose
that they get a little annoying when you want to run your whole test suite constantly.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Selective Testing&lt;/em&gt;.  When I&amp;rsquo;m working with code that doesn&amp;rsquo;t interact with Spark at all, sometimes I want to skip
the tests involving Spark so the tests run faster.  Spark is definitely fast enough to be used in unit tests, and its way
faster than Hadoop, but they do take a little longer than our other unit tests.  And some of our unit tests with Spark
crunch through millions of records, so they take a little longer.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Isolation is by far the most important, though the others are nice additions.  Isolation is also tricky, though.  We
could create a SparkContext for every test, but then we ran into these error messages:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[info]   org.jboss.netty.channel.ChannelException: Failed to bind to: /192.168.1.100:63726
[info]   at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:298)
[info]   at akka.remote.netty.NettyRemoteServer.start(Server.scala:53)
[info]   at akka.remote.netty.NettyRemoteTransport.start(NettyRemoteSupport.scala:89)
[info]   at akka.remote.RemoteActorRefProvider.init(RemoteActorRefProvider.scala:94)
[info]   at akka.actor.ActorSystemImpl._start(ActorSystem.scala:588)
[info]   at akka.actor.ActorSystemImpl.start(ActorSystem.scala:595)
[info]   at akka.actor.ActorSystem$.apply(ActorSystem.scala:111)
[info]   at spark.util.AkkaUtils$.createActorSystem(AkkaUtils.scala:40)
[info]   at spark.SparkEnv$.createFromSystemProperties(SparkEnv.scala:72)
[info]   at spark.SparkContext.&amp;lt;init&amp;gt;(SparkContext.scala:99)
[info]   ...
[info]   Cause: java.net.BindException: Address already in use
[info]   at sun.nio.ch.Net.bind(Native Method)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We found the solution to this problem in the unit tests in Spark:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; // To avoid Akka rebinding to the same port, since it doesn't unbind immediately on shutdown
 System.clearProperty(&quot;spark.master.port&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the unit tests for Spark itself, they use BeforeAndAfter to create a SparkContext for every test.  However, that
didn&amp;rsquo;t really work for us.  Not all of the tests in one test class needed a SparkContext, and we didn&amp;rsquo;t want to
make one when we didn&amp;rsquo;t need it (to keep those tests fast).  We generally use &lt;a href=&quot;http://www.scalatest.org/getting_started_with_fun_suite&quot;&gt;FunSuite from ScalaTest&lt;/a&gt;
for our tests, so we created a new &lt;code&gt;sparkTest&lt;/code&gt; method in a &lt;code&gt;SparkTestUtils&lt;/code&gt; trait that we could mix into our tests:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;object SparkTest extends org.scalatest.Tag(&quot;com.qf.test.tags.SparkTest&quot;)

trait SparkTestUtils extends FunSuite {
  var sc: SparkContext = _

  /**
   * convenience method for tests that use spark.  Creates a local spark context, and cleans
   * it up even if your test fails.  Also marks the test with the tag SparkTest, so you can
   * turn it off
   *
   * By default, it turn off spark logging, b/c it just clutters up the test output.  However,
   * when you are actively debugging one test, you may want to turn the logs on
   *
   * @param name the name of the test
   * @param silenceSpark true to turn off spark logging
   */
  def sparkTest(name: String, silenceSpark : Boolean = true)(body: =&amp;gt; Unit) {
    test(name, SparkTest){
      val origLogLevels = if (silenceSpark) SparkUtil.silenceSpark() else null
      sc = new SparkContext(&quot;local[4]&quot;, name)
      try {
        body
      }
      finally {
        sc.stop
        sc = null
        // To avoid Akka rebinding to the same port, since it doesn't unbind immediately on shutdown
        System.clearProperty(&quot;spark.master.port&quot;)
        if (silenceSpark) Logging.setLogLevels(origLogLevels)
      }
    }
  }
}

object SparkUtil {
  def silenceSpark() {
    setLogLevels(Level.WARN, Seq(&quot;spark&quot;, &quot;org.eclipse.jetty&quot;, &quot;akka&quot;))
  }

  def setLogLevels(level: org.apache.log4j.Level, loggers: TraversableOnce[String]) = {
    loggers.map{
      loggerName =&amp;gt;
        val logger = Logger.getLogger(loggerName)
        val prevLevel = logger.getLevel()
        logger.setLevel(level)
        loggerName -&amp;gt; prevLevel
    }.toMap
  }

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can then use this in our tests:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class OurAwesomeClassTest extends SparkTestUtils with ShouldMatchers {
  sparkTest(&quot;spark filter&quot;) {
    val data = sc.parallelize(1 to 1e6.toInt)
    data.filter{_ % 2 == 0}.count should be (5e5.toInt)
  }

  test(&quot;non-spark code&quot;) {
    val x = 17
    val y = 3
    OurAwesomeClass.plus(x,y) should be (20)
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we can run our tests within sbt:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//running all the tests
&amp;gt; test-only OurAwesomeClassTest
12/11/02 10:45:43 INFO slf4j.Slf4jEventHandler: Slf4jEventHandler started
12/11/02 10:45:44 INFO server.Server: jetty-7.5.3.v20111011
12/11/02 10:45:44 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:63859 STARTING
12/11/02 10:45:44 INFO server.Server: jetty-7.5.3.v20111011
12/11/02 10:45:44 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:63860 STARTING
[info] OurAwesomeClassTest:
[info] - spark filter
[info] - non-spark code
[info] Passed: : Total 2, Failed 0, Errors 0, Passed 2, Skipped 0
[success] Total time: 1 s, completed Nov 2, 2012 10:45:44 AM

//skipping the spark tests, by using the tag
&amp;gt; test-only OurAwesomeClassTest -- -l com.qf.test.tags.SparkTest
[info] OurAwesomeClassTest:
[info] - non-spark code
[info] Passed: : Total 1, Failed 0, Errors 0, Passed 1, Skipped 0
[success] Total time: 0 s, completed Nov 2, 2012 10:47:42 AM

//if we wanted, we could also *only* run the spark tests, though we never really use this
&amp;gt; test-only OurAwesomeClassTest -- -n com.qf.test.tags.SparkTest
12/11/02 10:47:37 INFO slf4j.Slf4jEventHandler: Slf4jEventHandler started
12/11/02 10:47:38 INFO server.Server: jetty-7.5.3.v20111011
12/11/02 10:47:38 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:63945 STARTING
12/11/02 10:47:38 INFO server.Server: jetty-7.5.3.v20111011
12/11/02 10:47:38 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:63946 STARTING
[info] OurAwesomeClassTest:
[info] - spark filter
[info] Passed: : Total 1, Failed 0, Errors 0, Passed 1, Skipped 0
[success] Total time: 1 s, completed Nov 2, 2012 10:47:38 AM
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You could modify the &lt;code&gt;sparkTest&lt;/code&gt; method to suit your needs, e.g., maybe you want to pass in the number of threads Spark
should use, or if you&amp;rsquo;d prefer to have the SparkContext directly passed into the body, using the
&lt;a href=&quot;https://wiki.scala-lang.org/display/SYGN/Loan&quot;&gt;Loan Pattern&lt;/a&gt;.  This met our needs for the moment, though, and we thought
it would be useful for others as well.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <id>http://blog.quantifind.com/posts/logging-post</id>
    <link type="text/html" rel="alternate" href="http://blog.quantifind.com/posts/logging-post"/>
    <title>Configuring Spark Logs</title>
    <published>2013-01-04T00:00:00-06:00</published>
    <updated>2013-01-04T00:00:00-06:00</updated>
    <author>
      <name></name>
    </author>
    <content type="html">&lt;p&gt;After you run a few Spark jobs, you&amp;rsquo;ll realize that Spark spits out a lot of logging messages.  At first, we found
this too distracting, so we turned off all Spark logs.  But that was too heavy-handed &amp;mdash; we always wanted to see &lt;em&gt;some&lt;/em&gt;
of the log messages, and of course, when we needed to debug something, we wanted everything.&lt;/p&gt;

&lt;!--more--&gt;


&lt;p&gt;So, we settled on the following configuration for log4j:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# make a file appender and a console appender
# Print the date in ISO 8601 format
log4j.appender.myConsoleAppender=org.apache.log4j.ConsoleAppender
log4j.appender.myConsoleAppender.layout=org.apache.log4j.PatternLayout
log4j.appender.myConsoleAppender.layout.ConversionPattern=%d [%t] %-5p %c - %m%n
log4j.appender.myFileAppender=org.apache.log4j.RollingFileAppender
log4j.appender.myFileAppender.File=spark.log
log4j.appender.myFileAppender.layout=org.apache.log4j.PatternLayout
log4j.appender.myFileAppender.layout.ConversionPattern=%d [%t] %-5p %c - %m%n



# By default, everything goes to console and file
log4j.rootLogger=INFO, myConsoleAppender, myFileAppender

# The noisier spark logs go to file only
log4j.logger.spark.storage=INFO, myFileAppender
log4j.additivity.spark.storage=false
log4j.logger.spark.scheduler=INFO, myFileAppender
log4j.additivity.spark.scheduler=false
log4j.logger.spark.CacheTracker=INFO, myFileAppender
log4j.additivity.spark.CacheTracker=false
log4j.logger.spark.CacheTrackerActor=INFO, myFileAppender
log4j.additivity.spark.CacheTrackerActor=false
log4j.logger.spark.MapOutputTrackerActor=INFO, myFileAppender
log4j.additivity.spark.MapOutputTrackerActor=false
log4j.logger.spark.MapOutputTracker=INFO, myFileAppender
log4j.additivty.spark.MapOutputTracker=false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This configuration file sends all the logs (from Spark and everything else) to a logfile, &lt;code&gt;spark.log&lt;/code&gt;.  That way we have the full logs
if we need to debug later on.  In addition, we log everything to the console except for some Spark messages
that are a little too verbose.  But, we keep some of the logs from Spark on the console.  In particular, starting
with &lt;a href=&quot;http://spark-project.org/release-0.6.0.html&quot;&gt;Spark 0.6&lt;/a&gt;, the logs include a message telling you when its running a
job, and exactly where in your code it got triggered:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;2012-11-01 13:00:10,647 [main] INFO  spark.SparkContext - Starting job: aggregate at MyAwesomeSourceFile.scala:86
2012-11-01 13:00:25,169 [main] INFO  spark.SparkContext - Job finished: aggregate at MyAwesomeSourceFile.scala:86, took 14.521604475 s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To make use of this configuration, first save it into a text file. Then if you add it to your classpath, it will automatically get picked up.
In particular, with Spark you can just drop it in the &lt;code&gt;conf&lt;/code&gt; directory, as that is already on the classpath by default.  Or, you can configure
it via code with:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; import org.apache.log4j.PropertyConfigurator;
 PropertyConfigurator.configure(propertyFile)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We should add that even though we&amp;rsquo;re sending most of the Spark logs to a logfile, those log messages are &lt;em&gt;great&lt;/em&gt;.  There are
all sorts of great information in there &amp;mdash; how many partitions are being used, the size of the serialized tasks, which
nodes they are running on, progress as individual tasks finish, etc.  They are invaluable for more detailed debugging
and optimization.  Sometimes we even watch the full logs as the job is running with &lt;code&gt;tail -f spark.log&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;There are a few log messages that only make it into the logfile, that we really wish we could also get in the console. For example,
sub-stages of a job get logged with DAGScheduler:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;2012-10-31 13:54:00,757 [DAGScheduler] INFO  spark.scheduler.DAGScheduler - Submitting Stage 1 (map at MyAwesomeSourceFile.scala:57), which has no missing parents
2012-10-31 13:54:00,785 [DAGScheduler] INFO  spark.scheduler.DAGScheduler - Submitting 52 missing tasks from Stage 1
...
2012-10-31 13:58:45,215 [DAGScheduler] INFO  spark.scheduler.DAGScheduler - Stage 1 (map at MyAwesomeSourceFile.scala:57) finished; looking for newly runnable stages
2012-10-31 13:58:45,216 [DAGScheduler] INFO  spark.scheduler.DAGScheduler - running: Set(Stage 2)
2012-10-31 13:58:45,217 [DAGScheduler] INFO  spark.scheduler.DAGScheduler - waiting: Set(Stage 0)
2012-10-31 13:58:45,217 [DAGScheduler] INFO  spark.scheduler.DAGScheduler - failed: Set()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, DAGScheduler emits so many more log messages, that we&amp;rsquo;d rather just have these messages in the logfile.  Maybe in
the future Spark will move those messages to another class, so they&amp;rsquo;re easy to filter from the rest of the messages? :)&lt;/p&gt;

&lt;p&gt;We know that there isn&amp;rsquo;t one &amp;ldquo;right way&amp;rdquo; to configure your logging, but we hope this is at least a useful starting point
for others.  Let us know if you come up with any useful variants of this, or even if you decide on a completely different
setup.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <id>http://blog.quantifind.com/posts/welcome</id>
    <link type="text/html" rel="alternate" href="http://blog.quantifind.com/posts/welcome"/>
    <title>Welcome to our tech blog</title>
    <published>2013-01-03T00:00:00-06:00</published>
    <updated>2013-01-03T00:00:00-06:00</updated>
    <author>
      <name></name>
    </author>
    <content type="html">&lt;p&gt;Our goal as a company is to create a data-driven product that is powerful but simple.  This simplicity is for the benefit of usability, but it happens to hide a lot of things that we have behind the scenes, including masses of data, trained algorithms, and useful bits of code.&lt;/p&gt;

&lt;p&gt;Some of these things we can share and some we can&amp;rsquo;t, and our aim with this blog is to share what we can.  This will also be a channel for us to give a bit back to the same open source community that makes our lives easier.&lt;/p&gt;

&lt;p&gt;Part of our aim here is purely self-interested: to attract talented engineers.  If you&amp;rsquo;re a developer or data scientist with an interest in what we do, please email jobs@quantifind.com.  We look forward to hearing from you.&lt;/p&gt;
</content>
  </entry>
  
 
</feed>
